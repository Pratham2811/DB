How primary writes

How oplog entries are constructed

How secondaries pull data

How replication pipelining works

Why lag happens

What timestamps matter

How sync sources are chosen

How rollback works

How elections depend on oplog similarity

How MongoDB guarantees consistency

This is deep backend + database internals ‚Äî exactly your style.

Let‚Äôs go step-by-step, real, brutal, internal.

üî• LEVEL 2 ‚Äî THE REPLICATION ENGINE (INSIDE MONGODB)

MongoDB replication has 6 stages:

Write on PRIMARY

PRIMARY creates oplog entry

SECONDARY tails (streams) the oplog

Apply replication pipeline

SECONDARY commits updates

SECONDARY acknowledges replication (WriteConcern)

We will go through each one.

1Ô∏è‚É£ STAGE 1 ‚Äî WRITE HAPPENS ON PRIMARY (THE STARTING POINT)

Client sends:

insertOne({name:"Prathamesh"})


PRIMARY does 2 things:

‚úî 1. Writes data to the WiredTiger storage engine

Document stored in data files

Indexes updated

Checkpoints used for durability

‚úî 2. Logs the operation to the OPLOG

This is the single most important thing:

Every write = 1 oplog entry.

PRIMARY is the single source of truth for :

data

oplog stream

cluster time

2Ô∏è‚É£ STAGE 2 ‚Äî PRIMARY CREATES OPLOG ENTRY (THE HEART OF REPLICATION)

Oplog = Operation Log
Location:

local.oplog.rs


It‚Äôs a capped collection (fixed size, circular buffer).

Example oplog entry:

{
  "ts": Timestamp(1728392020, 1),
  "t": 5,
  "h": NumberLong("930492309402"),
  "v": 2,
  "op": "i",
  "ns": "StorageApp.users",
  "o": { "_id": ObjectId("..."), "name": "Prathamesh" }
}


Let‚Äôs break this down:

‚úî ts ‚Äì logical timestamp

This is how secondaries know ordering.

‚úî t ‚Äì term

The term from the current primary‚Äôs election round.

‚úî op ‚Äì operation type

i = insert

u = update

d = delete

c = command (index creation, etc.)

n = no-op (used for heartbeats)

‚úî ns ‚Äì namespace (db.collection)

Ex: "StorageApp.users"

‚úî o ‚Äì the actual operation content

The content inserted/updated/deleted.

3Ô∏è‚É£ STAGE 3 ‚Äî SECONDARIES TAIL THE OPLOG (STREAMING)

SECONDARY connects to PRIMARY and starts a replication thread.

Internally, SECONDARY runs:

Find latest timestamp I have  
Request all ops after that timestamp  
Keep reading like a log tail


This is almost identical to:

tail -f oplog


SECONDARY continuously requests:

Give me the next operation after ts = X


PRIMARY returns a batch of oplog entries.

IMPORTANT:

Secondaries do not copy the whole database again and again.

They only replay oplog events.

4Ô∏è‚É£ STAGE 4 ‚Äî REPLICATION PIPELINE (3 INTERNAL THREADS)

MongoDB secondaries run a 3-thread replication engine:

[Fetch Stage] ‚Üí [Buffer Stage] ‚Üí [Apply Stage]


Let‚Äôs break these.

üßµ A. FETCH STAGE (oplog fetcher)

This thread pulls oplog entries from PRIMARY.

Connects to primary's oplog

Reads new entries

Streams them down

If PRIMARY is slow ‚Üí fetcher slows.
If network is slow ‚Üí fetcher slows (lag starts here).

üßµ B. BUFFER STAGE (oplog buffer)

Fetched oplog entries are placed into an internal in-memory queue.

It‚Äôs like a message queue:

New oplog events go in

Applier thread consumes them

If:

Apply thread slow ‚Üí buffer fills up

Buffer fills too much ‚Üí replication lag grows

Buffer full ‚Üí SECONDARY falls out of sync

üßµ C. APPLY STAGE (oplog applier)

This is where SECONDARY replays the operations.

For each oplog entry:

insert ‚Üí insert

update ‚Üí update

delete ‚Üí delete

Exactly the same operation PRIMARY did.

MongoDB guarantees:

Same order

Same timestamps

Same commit semantics

WHY THIS PIPELINE IS SEPARATE?

Because:

Fetching oplog is I/O bound

Applying oplog is CPU + disk bound

Buffering solves mismatch between the two speeds

This design maximizes replication speed.

5Ô∏è‚É£ STAGE 5 ‚Äî SECONDARY COMMITS UPDATES

After applying operations, the SECONDARY:

Updates its own data files

Updates its oplog timestamp

Updates majority commit point

Replies to PRIMARY about replication status

Now it is in sync.

6Ô∏è‚É£ STAGE 6 ‚Äî WRITE CONCERN (ACKING REPLICATION)

When a client writes with:

{ writeConcern: { w: "majority" } }


Flow is:

PRIMARY writes

PRIMARY logs oplog

SECONDARY applies oplog

SECONDARY confirms

PRIMARY returns success to client

This ensures no data loss even during failover.

üß† INTERNAL BEHAVIOR OF SECONDARY

SECONDARY tracks:

Last fetched timestamp

Last applied timestamp

Oplog sync source

Replication lag

Heartbeats from PRIMARY

SECONDARY may switch sync source if:

Primary slow

Network slow

Another secondary faster

This is automatic.

‚ö†Ô∏è WHAT IS REPLICATION LAG?

Difference between:

primary.oplog_latest_timestamp
vs
secondary.applied_latest_timestamp


Lag reasons:

Secondary disk slow

Primary very busy

Network poor

Secondary CPU overloaded

Large operations (bulk update/delete)

Lag is visible via:

rs.printSlaveReplicationInfo()

üß† FAILOVER + OPLOG + ELECTION RELATIONSHIP

This is crucial:

‚úî Winner of election = node with the most recent oplog

MongoDB checks:

‚ÄúWho has latest operations?‚Äù

‚ÄúWho is least behind?‚Äù

If a node falls behind:

It CANNOT become primary

Even if its priority is high

Oplog freshness = leadership eligibility.

‚ù§Ô∏è WHY OPLOG MAKES FAILOVER SAFE

Imagine PRIMARY dies.

Your cluster has:

PRIMARY
SECONDARY 1 (latest)
SECONDARY 2 (lagging)


Election algorithm chooses:

SECONDARY 1

because:

Largest oplog timestamp

Closest to data consistency

Safe to promote

SECONDARY 2 cannot win:

It will cause potential data loss

MongoDB prevents this

It votes NO for itself

This maintains strong data consistency.

‚ö†Ô∏è WHAT IF SECONDARY FALLS TOO FAR BEHIND?

If SECONDARY‚Äôs oplog becomes too old, and the PRIMARY rotated out old oplog entries:

SECONDARY cannot catch up.

Then SECONDARY enters:

‚ùå RECOVERING state
‚ùå Must perform initial sync again
‚ùå Copies entire DB from primary again

This is expensive, so good setups keep:

Big oplog size

Healthy hardware

Low replication lag

üî• LEVEL 2 SUMMARY (THE REAL ENGINE)

Here is your complete Level 2 mental model:

CLIENT WRITE  
   ‚Üì
PRIMARY  
   ‚Üì writes to data  
   ‚Üì writes to oplog  
SECONDARY  
   ‚Üì fetch oplog  
   ‚Üì buffer oplog  
   ‚Üì apply oplog  
   ‚Üì commit  
   ‚Üì send ack  


If PRIMARY fails:

Election chooses the secondary with latest oplog

That secondary becomes PRIMARY

Replication continues

Zero downtime

Oplog = the heartbeat of replication.
Everything depends on it.